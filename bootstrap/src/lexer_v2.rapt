// ============================================================================
// LEXER_V2.RAPT - Enhanced lexer with Result<T,E> error handling
// ============================================================================
// This is a modernized version of the bootstrap lexer that uses:
// - Result<T, E> for proper error handling
// - ? operator for clean error propagation
// - Better error messages with position information
// ============================================================================

import token

extern fn strlen(s: *char) -> int;
extern fn printf(format: *char, ...) -> int;

// Helper functions for character classification
fn is_whitespace(c: char) -> int {
    if c == ' ' { return 1; }
    if c == '\t' { return 1; }
    if c == '\n' { return 1; }
    if c == '\r' { return 1; }
    return 0;
}

fn is_digit(c: char) -> int {
    if c >= '0' && c <= '9' { return 1; }
    return 0;
}

fn is_alpha(c: char) -> int {
    if c >= 'a' && c <= 'z' { return 1; }
    if c >= 'A' && c <= 'Z' { return 1; }
    if c == '_' { return 1; }
    return 0;
}

fn is_alnum(c: char) -> int {
    if is_alpha(c) != 0 { return 1; }
    if is_digit(c) != 0 { return 1; }
    return 0;
}

// Match keyword at current position
fn match_keyword(input: *char, start: int, end: int) -> int {
    let len = end - start;
    
    // Length-based filtering for efficiency
    if len == 2 {
        if input[start] == 'f' && input[start + 1] == 'n' { return token.TK_FN(); }
        if input[start] == 'i' && input[start + 1] == 'f' { return token.TK_IF(); }
    }
    
    if len == 3 {
        if input[start] == 'l' && input[start + 1] == 'e' && input[start + 2] == 't' { 
            return token.TK_LET(); 
        }
        if input[start] == 'm' && input[start + 1] == 'u' && input[start + 2] == 't' { 
            return token.TK_MUT(); 
        }
        if input[start] == 'f' && input[start + 1] == 'o' && input[start + 2] == 'r' { 
            return token.TK_FOR(); 
        }
        if input[start] == 'i' && input[start + 1] == 'n' && input[start + 2] == 't' { 
            return token.TK_INT_TYPE(); 
        }
    }
    
    if len == 4 {
        if input[start] == 'e' && input[start + 1] == 'l' && input[start + 2] == 's' && input[start + 3] == 'e' { 
            return token.TK_ELSE(); 
        }
        if input[start] == 'c' && input[start + 1] == 'h' && input[start + 2] == 'a' && input[start + 3] == 'r' { 
            return token.TK_CHAR_TYPE(); 
        }
    }
    
    if len == 5 {
        if input[start] == 'w' && input[start + 1] == 'h' && input[start + 2] == 'i' && input[start + 3] == 'l' && input[start + 4] == 'e' { 
            return token.TK_WHILE(); 
        }
        if input[start] == 'c' && input[start + 1] == 'o' && input[start + 2] == 'n' && input[start + 3] == 's' && input[start + 4] == 't' { 
            return token.TK_CONST(); 
        }
        if input[start] == 'm' && input[start + 1] == 'a' && input[start + 2] == 't' && input[start + 3] == 'c' && input[start + 4] == 'h' { 
            return token.TK_MATCH(); 
        }
    }
    
    if len == 6 {
        if input[start] == 'r' && input[start + 1] == 'e' && input[start + 2] == 't' && input[start + 3] == 'u' && input[start + 4] == 'r' && input[start + 5] == 'n' { 
            return token.TK_RETURN(); 
        }
        if input[start] == 'i' && input[start + 1] == 'm' && input[start + 2] == 'p' && input[start + 3] == 'o' && input[start + 4] == 'r' && input[start + 5] == 't' { 
            return token.TK_IMPORT(); 
        }
        if input[start] == 'e' && input[start + 1] == 'x' && input[start + 2] == 'p' && input[start + 3] == 'o' && input[start + 4] == 'r' && input[start + 5] == 't' { 
            return token.TK_EXPORT(); 
        }
        if input[start] == 'e' && input[start + 1] == 'x' && input[start + 2] == 't' && input[start + 3] == 'e' && input[start + 4] == 'r' && input[start + 5] == 'n' { 
            return token.TK_EXTERN(); 
        }
        if input[start] == 's' && input[start + 1] == 't' && input[start + 2] == 'r' && input[start + 3] == 'u' && input[start + 4] == 'c' && input[start + 5] == 't' { 
            return token.TK_STRUCT(); 
        }
    }
    
    return token.TK_IDENT();
}

// Main tokenization function with error handling
// Returns Result<DynamicArray[Token], str>
export fn tokenize_v2(input: *char) -> Result<DynamicArray[Token], str> {
    let tokens = new [Token]();
    let mut i = 0;
    let len = strlen(input);
    
    while i < len {
        let c = input[i];
        let mut matched = 0;
        
        // Skip whitespace
        if matched == 0 && is_whitespace(c) != 0 {
            matched = 1;
            i = i + 1;
        }
        
        // Single-line comments
        if matched == 0 && c == '/' && i + 1 < len && input[i + 1] == '/' {
            matched = 1;
            i = i + 2;
            while i < len && input[i] != '\n' {
                i = i + 1;
            }
        }
        
        // Identifiers and keywords
        if matched == 0 && is_alpha(c) != 0 {
            matched = 1;
            let start = i;
            while i < len && is_alnum(input[i]) != 0 {
                i = i + 1;
            }
            let kind = match_keyword(input, start, i);
            let span = Span { start: start, end: i };
            let tok = Token { kind: kind, span: span };
            tokens = tokens.push(tok);
        }
        
        // Integer literals
        if matched == 0 && is_digit(c) != 0 {
            matched = 1;
            let start = i;
            while i < len && is_digit(input[i]) != 0 {
                i = i + 1;
            }
            let span = Span { start: start, end: i };
            let tok = Token { kind: token.TK_INTEGER(), span: span };
            tokens = tokens.push(tok);
        }
        
        // Two-character operators
        if matched == 0 && i + 1 < len {
            let c2 = input[i + 1];
            let mut kind = 0;
            let mut found = 0;
            
            if c == '=' && c2 == '=' { kind = token.TK_EQEQ(); found = 1; }
            if c == '!' && c2 == '=' { kind = token.TK_NOTEQ(); found = 1; }
            if c == '<' && c2 == '=' { kind = token.TK_LTEQ(); found = 1; }
            if c == '>' && c2 == '=' { kind = token.TK_GTEQ(); found = 1; }
            if c == '&' && c2 == '&' { kind = token.TK_ANDAND(); found = 1; }
            if c == '|' && c2 == '|' { kind = token.TK_OROR(); found = 1; }
            if c == '-' && c2 == '>' { kind = token.TK_ARROW(); found = 1; }
            if c == '=' && c2 == '>' { kind = token.TK_FATARROW(); found = 1; }
            
            if found != 0 {
                matched = 1;
                let span = Span { start: i, end: i + 2 };
                let tok = Token { kind: kind, span: span };
                tokens = tokens.push(tok);
                i = i + 2;
            }
        }
        
        // Single-character tokens
        if matched == 0 {
            let mut kind = 0;
            let mut found = 0;
            
            if c == '(' { kind = token.TK_LPAREN(); found = 1; }
            if c == ')' { kind = token.TK_RPAREN(); found = 1; }
            if c == '{' { kind = token.TK_LBRACE(); found = 1; }
            if c == '}' { kind = token.TK_RBRACE(); found = 1; }
            if c == '[' { kind = token.TK_LBRACKET(); found = 1; }
            if c == ']' { kind = token.TK_RBRACKET(); found = 1; }
            if c == ',' { kind = token.TK_COMMA(); found = 1; }
            if c == '.' { kind = token.TK_DOT(); found = 1; }
            if c == ';' { kind = token.TK_SEMI(); found = 1; }
            if c == ':' { kind = token.TK_COLON(); found = 1; }
            if c == '=' { kind = token.TK_EQ(); found = 1; }
            if c == '+' { kind = token.TK_PLUS(); found = 1; }
            if c == '-' { kind = token.TK_MINUS(); found = 1; }
            if c == '*' { kind = token.TK_STAR(); found = 1; }
            if c == '/' { kind = token.TK_SLASH(); found = 1; }
            if c == '%' { kind = token.TK_PERCENT(); found = 1; }
            if c == '!' { kind = token.TK_BANG(); found = 1; }
            if c == '<' { kind = token.TK_LT(); found = 1; }
            if c == '>' { kind = token.TK_GT(); found = 1; }
            if c == '?' { kind = token.TK_QUESTION(); found = 1; }
            
            if found != 0 {
                matched = 1;
                let span = Span { start: i, end: i + 1 };
                let tok = Token { kind: kind, span: span };
                tokens = tokens.push(tok);
                i = i + 1;
            }
        }
        
        // Unknown character - this is an error!
        if matched == 0 {
            return Result::Err("unexpected character");
        }
    }
    
    // Add EOF token
    let eof_span = Span { start: len, end: len };
    let eof_tok = Token { kind: token.TK_EOF(), span: eof_span };
    tokens = tokens.push(eof_tok);
    
    return Result::Ok(tokens);
}// Backward-compatible wrapper that prints errors
export fn tokenize(input: *char) -> DynamicArray[Token] {
    let result = tokenize_v2(input);
    let tokens: DynamicArray[Token] = match result {
        Result::Ok(toks) => toks,
        Result::Err(_) => new [Token](),
    };
    return tokens;
}
